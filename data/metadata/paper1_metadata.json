{
    "title": "Large Language Models: A Survey",
    "authors": [
        "Shervin Minaee 1, Tomas Mikolov 2, Narjes Nikzad 3, Meysam Chenaghlu 4",
        "Richard Socher 5, Xavier Amatriain 6, Jianfeng Gao 7",
        "1 Applied Scientist, Amazon Inc",
        "2 Senior Researcher, CIIRC CTU",
        "3 Cologne University of Applied Sciences"
    ],
    "abstract": "lot of attention due to their strong performance on a wide range of natural language tasks, since the release of ChatGPT in November 2022. LLMs\u2019 ability of general-purpose language understanding and generation is acquired by training billions of model\u2019s parameters on massive amounts of text data, as predicted by scaling laws [1], [2]. The research area of LLMs, while very recent, is evolving rapidly in many different ways. In this paper, we review some of the most prominent LLMs, including three popular LLM families (GPT, LLaMA, PaLM), and discuss their characteristics, contributions and limitations. We also give an overview of techniques developed to build, and augment LLMs. We then survey popular datasets prepared for LLM training, fine-tuning, and evaluation, review widely used LLM evaluation metrics, and compare the performance of several popular LLMs on a set of representative benchmarks. Finally, we conclude the paper by discussing open challenges and future research directions. I. I NTRODUCTION Language modeling is a long-standing research topic, dat- ing back to the 1950s with Shannon\u2019s application of informa- tion theory to human language, where he measured how well simple n-gram language models predict or compress natural language text [3]. Since then, statistical language modeling became fundamental to many natural language understanding and generation tasks, ranging from speech recognition, ma- chine translation, to information retrieval [4], [5], [6]. The recent advances on transformer-based large language models (LMs), pretrained on Web-scale text corpora, signifi- cantly extended the capabilities of language models (LLMs). For example, OpenAI\u2019s ChatGPT and GPT-4 can be used not only for natural language processing, but also as general task solvers to power Microsoft\u2019s Co-Pilot systems, for instance, can follow human instructions of complex new tasks per- forming multi-step reasoning when needed. LLMs are thus becoming the basic building block for the development of general-purpose AI agents or artificial general intelligence (AGI). As the field of LLMs is moving fast, with new findings, models and techniques being published in a matter of months or weeks [7], [8], [9], [10], [11], AI researchers and practi- tioners often find it challenging to figure out the best recipes to build LLM-powered AI systems for their tasks. This paper gives a timely survey of the recent advances on LLMs. We hope this survey will prove a valuable and accessible resource for students, researchers and developers. LLMs are large-scale, pre-trained, statistical language mod- els based on neural networks. The recent success of LLMs is an accumulation of decades of research and development of language models, which can be categorized into four waves that have different starting points and velocity: statistical lan- guage models, neural language models, pre-trained language models and LLMs. Statistical language models (SLMs) view text as a sequence of words, and estimate the probability of text as the product of their word probabilities. The dominating form of SLMs are Markov chain models known as the n-gram models, which compute the probability of a word conditioned on its immediate proceeding n \u2212 1 words. Since word probabilities are estimated using word and n-gram counts collected from text corpora, the model needs to deal with data sparsity (i.e., assigning zero probabilities to unseen words or n-grams) by using smoothing, where some probability mass of the model is reserved for unseen n-grams [12]. N-gram models are widely used in many NLP systems. However, these models are incomplete in that they cannot fully capture the diversity and variability of natural language due to data sparsity. Early neural language models (NLMs) [13], [14], [15], [16] deal with data sparsity by mapping words to low-dimensional continuous vectors (embedding vectors) and predict the next word based on the aggregation of the embedding vectors of its proceeding words using neural networks. The embedding vectors learned by NLMs define a hidden space where the semantic similarity between vectors can be readily computed as their distance. This opens the door to computing semantic similarity of any two inputs regardless their forms (e.g., queries vs. documents in Web search [17], [18], sentences in different languages in machine translation [19], [20]) or modalities (e.g., image and text in image captioning [21], [22]). Early NLMs are task-specific models, in that they are trained on task-specific data and their learned hidden space is task-specific. Pre-trained language models (PLMs), unlike early NLMs, are task-agnostic. This generality also extends to the learned arXiv:2402.06196v3  [cs.CL]  23 Mar 2025 --- Page 2 --- hidden embedding space. The training and inference of PLMs follows the pre-training and fine-tuning paradigm, where lan- guage models with recurrent neural networks [23] or trans- formers [24], [25], [26] are pre-trained on Web-scale unlabeled text corpora for general tasks such as word prediction, and then finetuned to specific tasks using small amounts of (labeled) task-specific data. Recent surveys on PLMs include [8], [27], [28]. Large language models mainly refer to transformer-based neural language models 1 that contain tens to hundreds of billions of parameters, which are pre-trained on massive text data, such as PaLM [31], LLaMA [32], and GPT-4 [33], as summarized in Table III. Compared to PLMs, LLMs are not only much larger in model size, but also exhibit stronger language understanding and generation abilities, and more importantly, emergent abilities that are not present in smaller- scale language models. As illustrated in Fig. 1, these emergent abilities include (1) in-context learning, where LLMs learn a new task from a small set of examples presented in the prompt at inference time, (2) instruction following, where LLMs, after instruction tuning, can follow the instructions for new types of tasks without using explicit examples, and (3) multi-step reasoning, where LLMs can solve a complex task by breaking down that task into intermediate reasoning steps as demonstrated in the chain-of-thought prompt [34]. LLMs can also be augmented by using external knowledge and tools [35], [36] so that they can effectively interact with users and environment [37], and continually improve itself using feedback data collected through interactions (e.g. via reinforcement learning with human feedback (RLHF)). Through advanced usage and augmentation techniques, LLMs can be deployed as so-called AI agents: artificial entities that sense their environment, make decisions, and take actions. Previous research has focused on developing agents for specific tasks and domains. The emergent abilities demonstrated by LLMs make it possible to build general-purpose AI agents based on LLMs. While LLMs are trained to produce responses in static settings, AI agents need to take actions to interact with dynamic environment. Therefore, LLM-based agents often need to augment LLMs to e.g., obtain updated information from external knowledge bases, verify whether a system action produces the expected result, and cope with when things do not go as expected, etc. We will discuss in detail LLM-based agents in Section IV. In the rest of this paper, Section II presents an overview of state of the art of LLMs, focusing on three LLM families (GPT, LLaMA and PaLM) and other representative models. Section III discusses how LLMs are built. Section IV discusses how LLMs are used, and augmented for real-world applications Sections V and VI review popular datasets and benchmarks for evaluating LLMs, and summarize the reported LLM evaluation results. Finally, Section VII concludes the paper by summa- rizing the challenges and future research directions. II. L ARGE LANGUAGE MODELS In this section we start with a review of early pre-trained neural language models as they are the base of LLMs, and 1Recently, several very promising non-transformer LLMs have been pro- posed, such as the LLMs based on structured state space models [29], [30]. See Section VII for more details. then focus our discussion on three families of LLMs: GPT, LlaMA, and PaLM. Table I provides an overview of some of these models and their characteristics. A. Early Pre-trained Neural Language Models Language modeling using neural networks was pioneered by [38], [39], [40]. Bengio et al. [13] developed one of the first neural language models (NLMs) that are comparable to n-gram models. Then, [14] successfully applied NLMs to machine translation. The release of RNNLM (an open source NLM toolkit) by Mikolov [41], [42] helped significantly popularize NLMs. Afterwards, NLMs based on recurrent neural networks (RNNs) and their variants, such as long short-term memory (LSTM) [19] and gated recurrent unit (GRU) [20], were widely used for many natural language applications including machine translation, text generation and text classification [43]. Then, the invention of the Transformer architecture [44] marks another milestone in the development of NLMs. By applying self-attention to compute in parallel for every word in a sentence or document an \u201cattention score\u201d to model the influence each word has on another, Transformers allow for much more parallelization than RNNs, which makes it possible to efficiently pre-train very big language models on large amounts of data on GPUs. These pre-trained language models (PLMs) can be fine-tuned for many downstream tasks. We group early popular Transformer-based PLMs, based on their neural architectures, into three main categories: encoder- only, decoder-only, and encoder-decoder models. Comprehen- sive surveys of early PLMs are provided in [43], [28]. 1) Encoder-only PLMs: As the name suggests, the encoder- only models only consist of an encoder network. These models are originally developed for language understanding tasks, such as text classification, where the models need to predict a class label for an input text. Representative encoder-only mod- els include BERT and its variants, e.g., RoBERTa, ALBERT, DeBERTa, XLM, XLNet, UNILM, as to be described below. BERT (Birectional Encoder Representations from Trans- formers) [24] is one of the most widely used encoder-only language models. BERT consists of three modules: (1) an embedding module that converts input text into a sequence of embedding vectors, (2) a stack of Transformer encoders that converts embedding vectors into contextual representation vectors, and (3) a fully connected layer that converts the representation vectors (at the final layer) to one-hot vectors. BERT is pre-trained uses two objectives: masked language modeling (MLM) and next sentence prediction. The pre-trained BERT model can be fine-tuned by adding a classifier layer for many language understanding tasks, ranging from text classification, question answering to language inference. A high-level overview of BERT framework is shown in Fig 3. As BERT significantly improved state of the art on a wide range of language understanding tasks when it was published, the AI community was inspired to develop many similar encoder-only language models based on BERT. RoBERTa [25] significantly improves the robustness of BERT using a set of model design choices and training strate- gies, such as modifying a few key hyperparameters, removing the next-sentence pre-training objective and training with much --- Page 3 --- Emerging Basic Augmented LLM Capabilities Reasoning Coding Comprehension Multilingual Tool utilization World knowledge Instruction following In-context learning Interacting with users Self-improvement Multi choice QA Wikipedia QA XNLI Crosslingual QA Crosslingual Tasks Translation Reading Comprehension Multi choice QA Boolean QA Simplification Summarization Function Calling API calling Logical Symbolic Common Sense Arithmetic Turn based Completion Task definition Few-shot Symbolic reference Pos/Neg example Step by step solving Tool planning Task decomposition Virtual acting Physical acting Knowledge base utilization Assignment planning Self-cirtisim Self-refinement Fig. 1: LLM Capabilities. larger mini-batches and learning rates. ALBERT [45] uses two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT: (1) splitting the embedding matrix into two smaller matrices, and (2) using repeating layers split among groups. DeBERTa (Decoding- enhanced BERT with disentangled attention) [26] improves the BERT and RoBERTa models using two novel techniques. The first is the disentangled attention mechanism, where each word is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices on their contents and relative positions, respectively. Second, an enhanced mask de- coder is used to incorporate absolute positions in the decoding layer to predict the masked tokens in model pre-training. In addition, a novel virtual adversarial training method is used for fine-tuning to improve models\u2019 generalization. ELECTRA [46] uses a new pre-training task, known as replaced token detection (RTD), which is empirically proven to be more sample-efficient than MLM. Instead of masking the input, RTD corrupts it by replacing some tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, a discriminative model is trained to predict whether a token in the corrupted input was replaced by a generated sample or not. RTD is more sample-efficient than MLM because the former is defined over all input tokens rather than just the small subset being masked out, as illustrated in Fig 4. XLMs [47] extended BERT to cross-lingual language models using two methods: (1) a unsupervised method that only relies on monolingual data, and (2) a supervised method that leverages parallel data with a new cross-lingual language model objective, as illustrated in Fig 5. XLMs had obtained state-of-the-art results on cross-lingual classification, unsuper- vised and supervised machine translation, at the time they were proposed. There are also encoder-only language models that leverage the advantages of auto-regressive (decoder) models for model training and inference. Two examples are XLNet and UNILM. XLNet [48] is based on Transformer-XL, pre-trained using a generalized autoregressive method that enables learning bidi- rectional contexts by maximizing the expected likelihood over all permutations of the factorization order. UNILM (UNIfied pre-trained Language Model) [49] is pre-trained using three types of language modeling tasks: unidirectional, bidirectional, and sequence-to-sequence prediction. This is achieved by employing a shared Transformer network and utilizing specific self-attention masks to control what context the prediction is conditioned on, as illustrated in Fig 6. The pre-trained model can be fine-tuned for both natural language understanding and generation tasks. 2) Decoder-only PLMs: Two of the most widely used decoder-only PLMs are GPT-1 and GPT-2, developed by OpenAI. These models lay the foundation to more powerful LLMs subsequently, i.e., GPT-3 and GPT-4. GPT-1 [50] demonstrates for the first time that good performance over a wide range of natural language tasks can be obtained by Generative Pre-Training (GPT) of a decoder-only Transformer model on a diverse corpus of unlabeled text in a self-supervised learning fashion (i.e., next word/token predic- tion), followed by discriminative fine-tuning on each specific downstream task (with much fewer samples), as illustrated in Fig 7. GPT-1 paves the way for subsequent GPT models, with each version improving upon the architecture and achieving better performance on various language tasks. GPT-2 [51] shows that language models are able to learn to perform specific natural language tasks without any explicit supervision when trained on a large WebText dataset consisting of millions of webpages. The GPT-2 model follows the model designs of GPT-1 with a few modifications: Layer normal- ization is moved to the input of each sub-block, additional layer normalization is added after the final self-attention block, --- Page 4 --- Paper Strcuture Early Pre-trained Language Models II Large Language Models A III HOW LLMS ARE BUILT A Data CleaningB Large Language Model FamiliesB Other Representative LLMsC Dominant LLM Architectures TokenizationsC Positional EncodingD Model Pre-trainingE Fine-tuning and Instruction TuningF AlignmentG Decoding StrategiesH I HOW LLMS ARE USED AND AUGMENTED A B LLM limitations Cost-Effective Training/Inference, Adaptation & CompressionI Using LLMs: Prompt Design and Engineering C Augmenting LLMs through external knowledge - RAG D Using External Tools E LLM Agents V  POPULAR DATASETS FOR LLMS A Datasets for Basic Tasks: language modeling/understanding/generation B  Datasets for Emergent: ICL, reasoning, instruction following C Datasets for Augmented: using external knowledge/tools VI  PROMINENT LLMS\u2019 PERFORMANCE ON BENCHMARKS A B VII CHALLENGES AND FUTURE DIRECTIONS A Smaller and more efficient Language Models LLMs\u2019 Performance on Different Tasks Popular Metrics for Evaluating LLMs B New Post-attention Architectural Paradigms C Multi-modal Models D Improved LLM Usage and Augmentation techniques D Security and Ethical/Responsible AI Fig. 2: The paper structure. initialization is modified to account for the accumulation on the residual path and scaling the weights of residual layers, vocabulary size is expanded to 50,25, and context size is increased from 512 to 1024 tokens. 3) Encoder-Decoder PLMs: In [52], Raffle et al. shows that almost all NLP tasks can be cast as a sequence-to-sequence generation task. Thus, an encoder-decoder language model, by design, is a unified model in that it can perform all natural language understanding and generation tasks. Representative encoder-decoder PLMs we will review below are T5, mT5, MASS, and BART. T5 [52] is a Text-to-Text Transfer Transformer (T5) model, where transfer learning is effectively exploited for NLP via an",
    "source_file": "paper1.pdf"
}